import{_ as a,c as o,o as e,a4 as s,bh as r,bi as n,bj as t,bk as i,bl as h,bm as d,bn as p,bo as c}from"./chunks/framework.CCJHkvW2.js";const x=JSON.parse('{"title":"MongoDB 集群","description":"","frontmatter":{},"headers":[],"relativePath":"db/mongodb.md","filePath":"db/mongodb.md","lastUpdated":1713833896000}'),l={name:"db/mongodb.md"},g=s('<h1 id="mongodb-集群" tabindex="-1">MongoDB 集群 <a class="header-anchor" href="#mongodb-集群" aria-label="Permalink to &quot;MongoDB 集群&quot;">​</a></h1><p>参考文献(<a href="https://www.bookstack.cn/read/linfenliang-mongodb/chapter8.md" target="_blank" rel="noreferrer">https://www.bookstack.cn/read/linfenliang-mongodb/chapter8.md</a>)(<a href="https://blog.csdn.net/mijichui2153/article/details/114779196" target="_blank" rel="noreferrer">https://blog.csdn.net/mijichui2153/article/details/114779196</a>)(<a href="https://blog.csdn.net/m0_65931372/article/details/125431385" target="_blank" rel="noreferrer">https://blog.csdn.net/m0_65931372/article/details/125431385</a>)</p><h2 id="主从集群" tabindex="-1">主从集群 <a class="header-anchor" href="#主从集群" aria-label="Permalink to &quot;主从集群&quot;">​</a></h2><p><img src="'+r+'" alt="An image"></p><p>使用主从复制集群有一个比较明显的缺陷：当主节点出现故障，比如停电或者死机等情况发生时，整个 MongoDB 服务集群就不能正常运作了。需要人工地去处理这种情况， 修复主节点之后再重启所有服务。当主节点一时难以修复时，我们也可以把其中 1 个从节点启动为主节点，在这个过程中就需要人工操作处理，而且需要停机操作，我们对外的服务会有一段空白时间， 给网站和其他应用的用户造成影响，所以说主从复制集群的容灾并不算太好。</p><h2 id="副本集集群" tabindex="-1">副本集集群 <a class="header-anchor" href="#副本集集群" aria-label="Permalink to &quot;副本集集群&quot;">​</a></h2><p>复制集中主要有三个角色：主节点（primary）、从节点（secondary）、仲裁者（非必需）。要组建复制集集群至少需要两个节点，主节点和从节点都是必需的， 主节点负责接受客户端的请求写入数据等操作，从节点则负责复制主节点上的数据，也可以提供给客户端读取数据的服务。仲裁者则是辅助投票修复集群。</p><p>复制集要完成数据复制以及修复集群依赖于两个基础的机制： oplog (operation log，操作日志）和心跳 (heartbeat)。oplog 让数据的复制成为可能，而“心跳”则监控节点的健康情况并触发故障转移。</p><p><img src="'+n+'" alt="An image"></p><p>复制集的心跳检测有助于发现故障进行自动选举和故障转移。默认情况下，每个复制集成员之间都会有心跳检测机制，每隔 2s发一次心跳包， ping 一次其他所有成员。 这样一来，系统可以弄清自己的健康状况。如果10s（5 次心跳时间）未收到某个节点的心跳，则认为该节点已宕机。如果宕机的节点为Primary，Secondary（前提是可被选为 Primary）会发起新的 Primary 选举。</p><p>只要每个节点都保持健康且有应答，说明复制集就很正常，没有故障发生。如果哪个点失去了响应，复制集就会采取相应的措施了。这时候复制集会去判断失去响应的是主节点还是从节点。</p><p>如果是多个从节点中的某一个从节点，则复制集不做任何处理，只是等待从节点重新上线。</p><p>如果是主节点挂掉了，则复制集就会开始进行选举了，选出新的主节点。</p><p>还有一种场景是复制集集群的主节点突然失去了其他大多数节点的心跳，主节点会把自己降级为从节点。这是为了防止网络原因让主节点和其他从节点断开时，其他的从节点中推举出了一个新的主节点，而原来的主节点又没降级的话，当网络恢复之后，复制集就出来了两个主节点。如果客户端继续运行，就会对两个主节点都进行读写操作，肯定复制集就混乱了。所以，当主节点失去多数节点的心跳时（自己不够半数），必须降级为从节点。 假设复制集内投票成员数量为 N，则大多数定义为 N/2 + 1，当复制集内存活成员数量不足大多数时，整个复制集将无法选举出 Primary，复制集将无法提供写服务，处于只读状态。</p><h3 id="选举机制" tabindex="-1">选举机制 <a class="header-anchor" href="#选举机制" aria-label="Permalink to &quot;选举机制&quot;">​</a></h3><p>如果主节点故障了，其余的节点就会选出一个新的主节点。选举可以由任意非主节点发起，然后根据优先级和 Bully 算法（核心就是评判谁的数据更新）选举出主节点。在选举出主节点之前，整个集群服务是只读的，不能执行写入操作。 非仲裁节点都有个优先级的配置，范围为 0~100 ，越大的值越优先成为主节点。默认情况下 1，如果是0，则不能成为主节点。选举机制会尽最大的努力让优先级最高的节点成为主节点 ，即使复制集中已经选举出了比较稳定的、但优先级比较低的主节点。优先级比较低的节点会短暂地作为主节点运行一段时间，但不能一直作为主节点。也就是说，如果优先级比较高的节点在 Bully 算法投票中没有胜出，复制集运行一段时间后会继续发起选举，直到优先级最高的节点成为主节点为止。由此可见，优先级的配置参数在选举机制中是很重要的，要么不设置，保持大家都是优先级 1 的公平状态，要么把可以把性能比较好的几台服务器设置的优先级高一些。这个可以根据实际的业务场景需求。</p><h3 id="数据回滚" tabindex="-1">数据回滚 <a class="header-anchor" href="#数据回滚" aria-label="Permalink to &quot;数据回滚&quot;">​</a></h3><p>不论哪一个从节点升级为主节点，新的主节点的数据都认定为 MongoDB 服务复制集 的最新数据，对其他节点（特别是原来的主节点）的操作都会回滚，即使之前故障的主节点恢复工作作为从节点加入集群之后。为了完成回滚，所有节点连接新的主节点后要重新同步。这些节点会查看自己的 oplog，找出其中新主节点中没有执行过的操作，然后向新主节点请求这些操作影响的文档的数据样本，替换掉自己的异常样本。正在执行重新同步的、之前故障的主节点被视为恢复中，在完成这个过程之前不能作为主节点的候选者。</p><h2 id="分片集群" tabindex="-1">分片集群 <a class="header-anchor" href="#分片集群" aria-label="Permalink to &quot;分片集群&quot;">​</a></h2><p>整体架构图 <img src="'+t+'" alt="An image"></p><p><img src="'+i+'" alt="An image"></p><p>部署架构图 <img src="'+h+'" alt="An image"></p><p><img src="'+d+'" alt="An image"></p><h2 id="mongos-路由" tabindex="-1">Mongos 路由 <a class="header-anchor" href="#mongos-路由" aria-label="Permalink to &quot;Mongos 路由&quot;">​</a></h2><p>mongos：即最上面的Router，mongos作为分片集群的入口所有的请求都由mongos来路由、分发、合并，这些动作对客户端驱动都是透明的。用户连接mongos就像是连接mongod一样使用。mongos通过缓存config server里面的元数据(metadata)确定每个分片有哪些数据，然后将读写请求分发到相应的某个或者某些shard。</p><p>①为保证集群的高可用，在集群中一般不只一个mongos入口。</p><p>②一种常用的模式是在每个应用服务器上部署一个mongos；这样可以减少应用程序和Router之言的网络延迟</p><p>③另外还有一种更适应于大型集群的模式。 即将mongos router放到专用的主机上，这样带来的好处是可以将应用服务器的数量与mongos实例的数量分离，进而可以更好的控制mongod实例所服务的连接数。</p><p>④一般来讲集群中的mongos数量没有什么数量上的限制。但是由于mongos经常会与Config Severs通信，因此在增加路由服务mongos的时候应该密切关注Config Servers的性能。如果发现性能有明显下降，在集群中适当限制mongos的数量可能是有益的。</p><div class="tip custom-block"><p class="custom-block-title">注</p><p>mongos应该是只有分片集群才会有的，一个普通的不分片的数据库实例客户端会直接连上mongod即可。</p></div><h2 id="config-servers-配置服务" tabindex="-1">Config Servers 配置服务 <a class="header-anchor" href="#config-servers-配置服务" aria-label="Permalink to &quot;Config Servers 配置服务&quot;">​</a></h2><p>Config Servers：配置服务器是保存集群中元信息的特殊mongod。换句话说就是给路由器(mongos)提供分片线索(理解为不同分片的索引表)。mongos通过查询configserver就知道当前存取的数据究竟在哪个或哪些分片(shard)，然后直接去访问对应的shard就好了。</p><div class="tip custom-block"><p class="custom-block-title">注</p><p>从3.4开始config server必须部署为一个副本集。</p></div><h2 id="mongod-数据库" tabindex="-1">mongod 数据库 <a class="header-anchor" href="#mongod-数据库" aria-label="Permalink to &quot;mongod 数据库&quot;">​</a></h2><p>mongod：一个服务器上的mongodb数据库实例通常我们称为mongod，在分片集群中一个mongod其实就对应一个分片(shard).</p><div class="tip custom-block"><p class="custom-block-title">注</p><p>1）注意mongos和mongod之间的关系，这是两个不同的入口。mongos可以统筹管理集群中的所有数据，mongod则是代表了“当前”数据库。举个例子，对于一个有5个服务器的mongo集群，通过mongos入口可以访问5个服务器的全部数据，通过mongod则只能访问到当前服务器的数据。</p><p>2）注意mongos路由不止一个。原因很简单，一个高可用的分布式集群方案必须保证服务时刻都可以正常高效运行，这里配置多个同样的mongos路由是为了预防当前路由出现故障而备用的。同样configserver也需要完全相同的副本分布在不同的服务器上；shard也一样可以设置多副本。</p></div><h2 id="chunk块" tabindex="-1">Chunk块 <a class="header-anchor" href="#chunk块" aria-label="Permalink to &quot;Chunk块&quot;">​</a></h2><p>对于一个shard mongodb也将数据划分成了更小的chunk(块)。每个chunk都唯一独占分片键的某段范围，片键值落在这个范围的文档也就落在这个chunk上。</p><p>chunk 是 MongoDB 在多个 shard 集群中迁移数据的最小单元。</p><div class="tip custom-block"><p class="custom-block-title">chunk的大小会影响那些东西</p><p>1）如果chunk过小可能会导致chunk数量的激增。他可以保证你的数据均匀的分布在 shard 集群中但是可能会导致频繁的数据迁移。这将加重 mongos 层面上的操作。</p><p>2）大的 chunk 会减少数据迁移，减轻网络负担，降低在 mongos 路由层面上的负载，但弊端是有可能导致数据在 shard 集群中分布的不均匀。</p></div><p><img src="'+p+'" alt="An image"></p><h2 id="分片方式" tabindex="-1">分片方式 <a class="header-anchor" href="#分片方式" aria-label="Permalink to &quot;分片方式&quot;">​</a></h2><p>Mongodb为我们提供了两种分片方式，分别是 <code>范围分片(Range based sharding)</code> 和 <code>Hash分片(hash based sharding)</code>。如下图分别为范围分片和hash分片，从图中很容易理解中两种分片方式的含义。</p><p><img src="'+c+'" alt="An image"></p><h3 id="范围分片-range-based-sharding" tabindex="-1">范围分片(Range based sharding) <a class="header-anchor" href="#范围分片-range-based-sharding" aria-label="Permalink to &quot;范围分片(Range based sharding)&quot;">​</a></h3><p>根据片键值所处的范围将一众数据存储到不同的分片上；实际上在shard内部这些数据又会不同chunk上。chunk存储在哪个shard、每个chunk存储数据范围的信息会存储在config server中。</p><p>优点： 提供更高效的范围查询。例如查找x的值在100~200之间的数据，mongos就可以根据config server中存储的元数据直接定位到指定shard的指定chunk，将请求直接转发到响应分片，一步到位。</p><p>缺点：数据分布不平衡。 如果片键所在字段是线性的(递增or递减)，一定时间内所有请求都会落到某个固定的chunk(亦shard)中。其状况就是一个分片承载了集群几乎所有的数据，并没有起到写扩展的效果。</p><h3 id="hash分片-hash-based-sharding" tabindex="-1">Hash分片(hash based sharding) <a class="header-anchor" href="#hash分片-hash-based-sharding" aria-label="Permalink to &quot;Hash分片(hash based sharding)&quot;">​</a></h3><p>对key(片键值)进行hash然后决定其存放于哪个分片，这种方式又称为随机分片。</p><p>优点：可以选用任何字段作为hash key理论上讲无论是有序的字段、string，还是自动生成的_id最后的分片效果都很不错，能将数据均匀的分散到各个服务器上。这样就很好的实现了写扩展。</p><p>缺点：不能支持高效的范围查询。哈希值的随机性使得数据近乎随机的分布在不同的chunk/shard中，正是由于这种随机性为了返回范围查询的结果需要针对每个值分别请求不同的分片。</p><p>总结：总的来说范围分片和hash分片正好是互补的。范围分片支持更高效的范围查询，但存在数据分布不均匀的问题。hash分片则恰恰相反，它以牺牲范围查询为代价保证了数据的均衡。这里要提一下的是范围分片这种数据分布不均衡所带来的问题很有可能会大于其带来的积极作用。</p>',53),m=[g];function b(u,k,_,f,q,v){return e(),o("div",null,m)}const y=a(l,[["render",b]]);export{x as __pageData,y as default};
